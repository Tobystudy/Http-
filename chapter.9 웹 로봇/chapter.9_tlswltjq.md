# 웹 로봇
HTTP 사용자 에이전트
방식에 따라 크롤러, 스파이더, 웜, 봇 등으로 불린다.
#### 웹 로봇의 예시
- 주식시장 서버에 매 분 HTTP/GET요청을 보내고 데이터를 활용해 그래프를 생성하는 주식 그래프 로봇
- WWW을 돌면서 페이지의 개수를 세고 페이지의 크기 언어, 타입, 미디어 타입등을 기록하는 웹 통계 조사 로봇
- 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
- 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 가격을 비교하는 가격 비교 로봇
## 크롤러와 크롤링

웹 크롤러는 웹 페이지 하나를 가져오고 그 다음 그 페이지에서 가리키는 모든 웹 페이지를 가져오는 과정을 재귀적으로 반복해 웹을 순환하는 로봇이다.
#### 우선 가져올 페이지 (root set)
크롤러를 풀어놓기 전 출발지점이 필요하다.
크롤러가 방문을 시작하는 URL의 초기 집합을 루트 집합(root set)이라고 한다.
> 일반적으로 좋은 루트집합 = 크고 인기 있는 웹 사이트, 새로생성된 페이지 목록, 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록

#### 링크 파싱
크롤러가 HTML을 검색하고 내용에 있는 URL를 파싱 하고 절대 URL로 변환, 방문 목록에 추가한다.
### 순환 피하기
로봇이 웹을 크롤링할 때 루프에 빠지지 않도록 조심해야한다.
순환을 피하기 위해서 방문한곳을 알아야한다.
#### 순환이 해로운 이유
1. 루프에 빠져 빙빙 돌 수 있다.
   같은 페이지를 반복해서 가져오다 시간을 허비하게된다.
   네트워크 대역폭을 다 차지하고 어떤 페이지도 가져올 수 없게 될 수 있음.
2. 같은 페이지를 계속 가져온다는것은 웹 서버에 부담을 주는것. 크롤러의 빠르고 동일한 요청은 웹 서비를 마비시킬수도 있고 이는 법적인 문제의 소지가 된다.
3. 앞선 문제가 실질적으로 문제되지 않더라도 중복된 많은 페이지를 가져오는것은 쓸모없는 행위.
#### 어떻게 해결할까
엄청나게 넓은 웹을 크롤링하려면 수십억의 URL을 방문하고 방문한곳을 추적해야한다.
따라서 속도와 메모리 측면에서 효율적인 자료구조를 사용한다.
**대표적인 기법**
- 트리와 해시 테이블
- 느슨한 존재 비트맵
- 체크포인트
- 파티셔닝
### 그래도 순환에 빠질 수 있는 케이스
URL 어떤 리소스를 가르키는 문자열 따라서 달라보이는 URL이 같은 리소스를 가리킬 수 있다.
#### URL 정규화 하기
1. 포트 번호가 명시되지 않았다면 호스트명에 :80을 추가한다.
2. 모든 %** 이스케이핑된 문자를 대응되는 문자로 변환한다.(URL인코딩 된것을 되돌리기)
3. \#태그들을 제거한다.
### 파일 시스템 링크 순환
파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 종류의 순환을 유발할 수 있다.
### 동적 가상 웹 공간
동적 콘텐츠로 인해서 무한히 깊어지는 에플리케이션에도 빠질 수 있다.
e. g. 달력의 다음 달 -> 다다음달 -> 다다다음달 콘텐츠
### 루프와 중복 피하기
휴리스틱의 집합이 필요하다.
>휴리스틱(Heuristics)이란,**문제를 해결하거나 불확실한 사항에 대해 판단을 내릴 필요가 있지만,** **명확한 실마리가 없을 경우에 사용하는 편의적 발견적인 방법**

자율적인 크롤러의 휴리스틱은  로봇의 구현자에게 트레이드오프를 만든다. 약간의 손실을 유발한다는것. 이는 불필요한 컨텐츠를 걸러낼 수도 있지만 유효한 컨텐츠도 걸러버릴 수 있다.

웹은 로봇이 문제를 일으킬 가능성이로 차있기에 다음 기법을 사용해 올바르게 동작하게 돕는다.
- URL 정규화
- 너비 우선 크롤링
- 스로틀링
- URL 크기제한
- URL/사이트 블랙리스트
- 패턴 발견
- 콘텐츠 지문
- 사람의 모니터링
## 로봇의 HTTP
로봇도 HTTP를 사용하는 여타 클라이언트 프로그램과 다르지 않다. 따라서 HTTP명세를 지켜가며 적절한 요청헤더를 사용해야한다.
### 요청 헤더 식별하기
로봇들은 HTTP를 최소한으로만 지원하지만 신원 식별을 위한 헤더를 구현하고 전송한다.
잘못 된 크롤러의 소유자를 찾아낼 때와 서버에게 로봇이 어떤 조류의 컨텐츠를 다룰 수 있는지에 대한 약간의 정보를 주려할 때 모두 유용하다.
- User-Agent : 서버에 요청을 만든 로봇의 이름
- From : 로봇의 사용자/관리자의 이메일 주소
- Accept : 서버에게 어떤 미디어 타입을 보내도 되는지 말해줌
- Referer : 현재의 요청 URL을 포함한 문서의 URL
### 가상호스팅
가상 호스팅을 이용하는 경우 Host 헤더가 없다면
가상 호스팅중인 두 사이트 중 기본으로 제공하도록 설정된 사이트의 리소스를 제공하게될 것
### 조건부 요청
로봇이 엄청난 양의 요청을 시도한다는 것을 생각해 보면 로봇이 검색하는 콘텐츠의 양을 최소화 하는 것도 상당히 의미있는 일이다.
검색엔진 로봇의 경우 조건부 요청을 이용한다면 오직 변경 되었을 때만 콘텐츠를 가져오게 할 수 있다.
### 응답 다루기
좀 더 상호작용을 잘 해 몇몇 기능(조건부요청 등)을 이용하려면 여러 종류의 HTTP 응답을 다룰 줄 알아야한다.
## 부적절한 동작의 로봇들
#### 폭주하는 로봇
로봇은 사람보다 훨씬 빠르게 HTTP요청을 만들 수 있다 논리적 에러를 갖거나 순환에 빠져있다면 웹서버에 과부하를 유발해 서비스를 마비시킬 수 있으므로 보호장치를 만들어야한다
#### 오래된 URL
로봇이 오래된 URL목록을 비우지 않고 해당 URL로 요청을 보낼 수 있다.
#### 길고 잘못 된 URL
이상하고 오래되며 순환에 빠지거나 프로그램상 오류로 크고, 의미없는 URL을 요청할 수 있다.
#### 호기심이 지나친 로봇
어떤 로봇들은 사적 데이터에대한 URL을 획득해 원치않는 리소스에 접근할 수 있다.
인터넷에 URL이 존재하는 이상 완전히 사적인 리소스는 없다.
#### 동적 게이트웨이 접근
로봇은 리소스를 알고 접근하지않는다.
동적 콘텐츠 게이트웨이에 요청 -> 비싼 비용 == 맘에 안듬
## 로봇 차단하기
대상을 크롤링하기 전 robots.txt를 요청해 먼저 확인을 한다.

## 풀 텍스트 색인
- 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스. 생성 이후에는 검색할 필요가 없다.
